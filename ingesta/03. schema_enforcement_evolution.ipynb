{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f8ba63a-6d3b-43e8-b618-3c2cac5fb95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Schema Enforcement y Schema Evolution en Databricks\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En el mundo de los datos, los esquemas definen la estructura de nuestras tablas: nombres de columnas, tipos de datos, restricciones, etc. Cuando trabajamos con pipelines de datos, especialmente en entornos donde los datos provienen de múltiples fuentes o evolucionan con el tiempo, necesitamos mecanismos para manejar cambios en la estructura de los datos.\n",
    "\n",
    "Databricks proporciona dos conceptos fundamentales para manejar esquemas de manera robusta:\n",
    "\n",
    "1. **Schema Enforcement**: Validación estricta de esquemas durante operaciones de escritura\n",
    "2. **Schema Evolution**: Adaptación automática o controlada de esquemas cuando cambian los datos\n",
    "\n",
    "Esta notebook explica ambos conceptos y muestra ejemplos prácticos aplicados a nuestras tablas de ingesta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bcec5cd-a287-4ec5-8575-d91c9ee5ae04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 1: Schema Enforcement\n",
    "\n",
    "### ¿Qué es Schema Enforcement?\n",
    "\n",
    "Schema Enforcement es una **validación estricta** que Databricks aplica automáticamente cuando escribimos datos en tablas Delta Lake. Esta validación asegura la calidad de los datos verificando que:\n",
    "\n",
    "- Todas las columnas que se intentan insertar existen en la tabla destino\n",
    "- Los tipos de datos de las columnas coinciden exactamente con el esquema definido\n",
    "- No se permiten inserciones que violen la estructura esperada\n",
    "\n",
    "### Comportamiento por defecto en Delta Lake\n",
    "\n",
    "Por defecto, **todas las tablas Delta Lake tienen Schema Enforcement habilitado**. Esto significa que cualquier operación de escritura que no cumpla con el esquema exacto de la tabla fallará.\n",
    "\n",
    "**Reglas de validación:**\n",
    "\n",
    "1. **Columnas existentes**: Todas las columnas en los datos fuente deben existir en la tabla destino\n",
    "2. **Tipos de datos**: Los tipos de datos deben coincidir exactamente\n",
    "3. **Columnas nuevas**: No se permiten columnas adicionales (generan error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0ef194b-0480-4aa2-b90f-6954ef4b5ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Ejemplo de Schema Enforcement\n",
    "\n",
    "```sql\n",
    "-- Supongamos que tenemos esta tabla:\n",
    "CREATE TABLE mi_tabla (\n",
    "  id INT,\n",
    "  nombre STRING,\n",
    "  fecha DATE\n",
    ");\n",
    "\n",
    "-- Esta inserción funciona (cumple con el esquema):\n",
    "INSERT INTO mi_tabla VALUES (1, 'Juan', '2024-01-01');\n",
    "\n",
    "-- Esta inserción FALLA (columna adicional):\n",
    "INSERT INTO mi_tabla VALUES (1, 'Juan', '2024-01-01', 'extra'); -- ERROR\n",
    "\n",
    "-- Esta inserción FALLA (tipo de dato incorrecto):\n",
    "INSERT INTO mi_tabla VALUES ('1', 'Juan', '2024-01-01'); -- ERROR: id debe ser INT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "679d2884-b660-4628-9a53-c839a68a72d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Consideraciones importantes\n",
    "\n",
    "**Ventajas:**\n",
    "- ✅ **Calidad de datos**: Previene inserciones incorrectas\n",
    "- ✅ **Consistencia**: Mantiene la estructura esperada\n",
    "- ✅ **Detección temprana**: Errores se detectan inmediatamente\n",
    "\n",
    "**Desventajas:**\n",
    "- ❌ **Rigidez**: No permite cambios naturales en los datos\n",
    "- ❌ **Mantenimiento**: Requiere actualizaciones manuales del esquema\n",
    "- ❌ **Flexibilidad limitada**: Problemas con datos semi-estructurados\n",
    "\n",
    "### Schema Enforcement en streaming\n",
    "\n",
    "Para streams continuos, Schema Enforcement es crítico porque:\n",
    "- Los streams esperan un esquema consistente\n",
    "- Cambios inesperados pueden romper el procesamiento\n",
    "- La validación ocurre en cada micro-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e45867-e7b9-42c8-b60a-28dcb58ed4e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 2: Schema Evolution\n",
    "\n",
    "### ¿Qué es Schema Evolution?\n",
    "\n",
    "Schema Evolution es la **capacidad de un sistema para adaptarse a cambios en la estructura de los datos** sin requerir intervención manual completa. En Databricks, esto permite:\n",
    "\n",
    "- Agregar nuevas columnas automáticamente\n",
    "- Cambiar tipos de datos de manera controlada\n",
    "- Renombrar columnas sin perder datos\n",
    "- Adaptarse a esquemas que evolucionan naturalmente\n",
    "\n",
    "### Tipos de cambios en Schema Evolution\n",
    "\n",
    "Los cambios más comunes que maneja Schema Evolution incluyen:\n",
    "\n",
    "1. **Nuevas columnas**: Campos adicionales que aparecen en los datos\n",
    "2. **Renombrado de columnas**: Cambios en nombres de campos\n",
    "3. **Eliminación de columnas**: Campos que dejan de existir\n",
    "4. **Cambio de tipos**: Modificaciones en tipos de datos (ampliación de tipos de datos)\n",
    "5. **Cambios de tipos arbitrarios**: Transformaciones más complejas\n",
    "\n",
    "### Type Widening (Ampliación de tipos de datos)\n",
    "\n",
    "Type widening es una característica avanzada de Schema Evolution que permite **cambiar tipos de datos a tipos más amplios sin reescribir los archivos de datos subyacentes**. Esta funcionalidad está disponible en **Databricks Runtime 15.4 LTS y superior**.\n",
    "\n",
    "#### ¿Qué permite type widening?\n",
    "\n",
    "Type widening permite cambiar tipos de datos siguiendo reglas específicas de **ampliación** (de tipos más restrictivos a más amplios):\n",
    "\n",
    "| Tipo fuente | Tipos más amplios soportados |\n",
    "|-------------|-----------------------------|\n",
    "| byte       | short, int, long, decimal, double |\n",
    "| short      | int, long, decimal, double |\n",
    "| int        | long, decimal, double |\n",
    "| long       | decimal |\n",
    "| float      | double |\n",
    "| decimal    | decimal con mayor precisión y escala |\n",
    "| date       | timestampNTZ |\n",
    "\n",
    "**Importante**: Los cambios de tipos enteros (`byte`, `short`, `int`, `long`) a `decimal` o `double` deben hacerse **manualmente** para evitar promociones accidentales.\n",
    "\n",
    "#### Ejemplo de type widening\n",
    "\n",
    "```sql\n",
    "-- Habilitar type widening en la tabla\n",
    "ALTER TABLE mi_tabla SET TBLPROPERTIES ('delta.enableTypeWidening' = 'true');\n",
    "\n",
    "-- Cambiar tipo de columna sin reescribir datos\n",
    "ALTER TABLE mi_tabla ALTER COLUMN edad TYPE INT;  -- de SMALLINT a INT\n",
    "ALTER TABLE mi_tabla ALTER COLUMN precio TYPE DECIMAL(10,2);  -- de DECIMAL(5,2) a DECIMAL(10,2)\n",
    "```\n",
    "\n",
    "#### Type widening con schema evolution automática\n",
    "\n",
    "Type widening se puede combinar con schema evolution para **cambios automáticos de tipos** durante la ingesta:\n",
    "\n",
    "```python\n",
    "# Escritura con schema evolution y type widening\n",
    "(df.write\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(\"mi_tabla\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Condiciones para type widening automático:**\n",
    "- Schema evolution habilitado (`mergeSchema=true`)\n",
    "- Type widening habilitado en la tabla destino\n",
    "- El tipo fuente es más amplio que el tipo destino\n",
    "- El cambio está soportado por las reglas de type widening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a91fd6b-2102-46c6-ab0c-1fa9ef7f253b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Arquitectura de Schema Evolution en Databricks\n",
    "\n",
    "Databricks maneja Schema Evolution a través de **4 componentes independientes**:\n",
    "\n",
    "1. **Conectores**: Componentes que ingestan datos externos\n",
    "2. **Parsers de formato**: Funciones que decodifican formatos raw\n",
    "3. **Engines**: Motores de procesamiento (Structured Streaming)\n",
    "4. **Datasets**: Almacenes finales (Delta tables, views, etc.)\n",
    "\n",
    "Cada componente maneja Schema Evolution de manera independiente, requiriendo configuración específica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2239fc8f-72da-48c1-a577-890a6b22c988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Schema Evolution por componente\n",
    "\n",
    "#### Auto Loader\n",
    "- ✅ **Nuevas columnas**: Soportado (con restart)\n",
    "- ✅ **Renombrado**: Soportado (tratado como nueva columna + NULL)\n",
    "- ❌ **Tipos widening**: No soportado\n",
    "- ❌ **Tipos arbitrarios**: No soportado\n",
    "\n",
    "#### Delta Connector\n",
    "- ✅ **Nuevas columnas**: Soportado\n",
    "- ✅ **Renombrado**: Soportado con configuración específica\n",
    "- ✅ **Tipos widening**: Soportado con configuración\n",
    "- ✅ **Tipos arbitrarios**: Soportado (requiere rewrite)\n",
    "\n",
    "#### Structured Streaming\n",
    "- ✅ **Todos los cambios**: Soportados (requieren restart del stream)\n",
    "\n",
    "#### Delta Tables\n",
    "- ✅ **Nuevas columnas**: Auto-evolución con `mergeSchema`\n",
    "- ✅ **Renombrado**: Con `ALTER TABLE` + column mapping\n",
    "- ✅ **Tipos widening**: Con table property específica\n",
    "- ✅ **Tipos arbitrarios**: Con `overwriteSchema`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09d54fb5-d16c-41f2-bbd5-880c4cf9a25d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Consideraciones importantes\n",
    "\n",
    "**Cuándo usar Schema Evolution:**\n",
    "- ✅ Datos semi-estructurados que evolucionan naturalmente\n",
    "- ✅ Pipelines que deben ser resilientes a cambios\n",
    "- ✅ Equipos con ciclos de desarrollo rápidos\n",
    "\n",
    "**Cuándo evitar Schema Evolution:**\n",
    "- ❌ Esquemas estables con contratos estrictos\n",
    "- ❌ Requisitos de calidad que demandan validación estricta\n",
    "- ❌ Entornos regulados con cambios controlados\n",
    "\n",
    "**Mejores prácticas:**\n",
    "- Configurar evolución por operación, no globalmente\n",
    "- Monitorear cambios de esquema\n",
    "- Documentar evoluciones importantes\n",
    "- Realizar pruebas exhaustivas antes de aplicar cambios en producción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "809b5580-483f-4f73-b43c-4993fbb2c7ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 3: Ejemplo práctico - Schema Evolution en carga JDBC\n",
    "\n",
    "Basado en el notebook `01. carga_full_jdbc.ipynb`, vamos a mostrar cómo aplicar Schema Evolution a las tablas cargadas desde PostgreSQL.\n",
    "\n",
    "### Escenario\n",
    "\n",
    "Supongamos que nuestra tabla `accounts` en PostgreSQL agrega una nueva columna `account_status` que indica si la cuenta está activa. Queremos que nuestra tabla Delta evolucione automáticamente para incluir esta columna.\n",
    "\n",
    "### Código de ejemplo con Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2719eb2-0bb0-4a1d-945b-14923a6cde8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# La configuración es igual que el notebook original\n",
    "# Acá suponemos que en df_strings tenemos una nueva columna account_status que apareció en la tabla origen\n",
    "# ESCRITURA CON SCHEMA EVOLUTION\n",
    "# La diferencia clave: mergeSchema=\"true\" permite agregar nuevas columnas\n",
    "(df_strings.write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"replaceWhere\", f\"fecha_extraccion = '{today}'\")\n",
    "    .option(\"mergeSchema\", \"true\")  # <-- HABILITA SCHEMA EVOLUTION\n",
    "    .saveAsTable(target)\n",
    ")\n",
    "\n",
    "print(\"Tabla procesada con Schema Evolution habilitado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d28254bb-eeb2-42c7-8269-8cb144a3ebf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿Qué cambia con Schema Evolution?\n",
    "\n",
    "**Sin Schema Evolution (por defecto):**\n",
    "- Si `accounts` agrega `account_status`, la escritura falla\n",
    "- Error: \"Cannot add column account_status to table accounts\"\n",
    "\n",
    "**Con Schema Evolution (`mergeSchema=true`):**\n",
    "- La columna `account_status` se agrega automáticamente\n",
    "- Registros existentes tienen `NULL` para esa columna\n",
    "- Nuevos registros incluyen los valores de `account_status`\n",
    "\n",
    "### Configuración adicional para tipos de cambios avanzados\n",
    "\n",
    "Si necesitamos manejar cambios más complejos, podemos configurar propiedades adicionales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9b7274d-9de4-4111-9da3-ea54f56220f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Habilitar type widening para cambios de tipos\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {target} SET TBLPROPERTIES (\n",
    "    'delta.typeWidening.enabled' = 'true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Habilitar column mapping para renombrado\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {target} SET TBLPROPERTIES (\n",
    "    'delta.columnMapping.mode' = 'name'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3cb1632-c7e7-405c-a7ef-09211182fcd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Parte 4: Schema Evolution en Auto Loader\n",
    "\n",
    "### ¿Cómo funciona Schema Evolution en Auto Loader?\n",
    "\n",
    "Auto Loader tiene un **sistema sofisticado de inferencia y evolución de esquemas** que permite manejar cambios en la estructura de datos de manera automática.\n",
    "\n",
    "#### Inferencia inicial de esquema\n",
    "\n",
    "Cuando Auto Loader procesa datos por primera vez:\n",
    "- **Muestrea** los primeros 50 GB o 1000 archivos (lo que ocurra primero)\n",
    "- **Infiera el esquema** basado en el formato del archivo:\n",
    "  - **JSON/CSV/XML**: Todas las columnas se infieren como `STRING`\n",
    "  - **Parquet/Avro**: Usa los tipos codificados en el esquema del archivo\n",
    "- **Almacena** la información del esquema en el directorio `_schemas` dentro de `cloudFiles.schemaLocation`\n",
    "\n",
    "#### Modos de evolución de esquema\n",
    "\n",
    "Auto Loader soporta diferentes modos de evolución configurados con `cloudFiles.schemaEvolutionMode`:\n",
    "\n",
    "| Modo | Comportamiento |\n",
    "|------|----------------|\n",
    "| `addNewColumns` (default) | **Stream falla**. Nuevas columnas se agregan al esquema. Tipos de columnas existentes no evolucionan. |\n",
    "| `rescue` | **Esquema nunca evoluciona**. Nuevas columnas van a la columna de datos rescatados. |\n",
    "| `failOnNewColumns` | **Stream falla**. No reinicia hasta actualizar el esquema manualmente. |\n",
    "| `none` | **No evoluciona esquema**. Nuevas columnas se ignoran (a menos que se configure `rescuedDataColumn`). |\n",
    "\n",
    "**Nota**: `addNewColumns` es el default cuando no se proporciona esquema, pero `none` es el default cuando se proporciona un esquema.\n",
    "\n",
    "#### Proceso de evolución\n",
    "\n",
    "1. **Detección**: Auto Loader detecta nuevas columnas en el micro-batch actual\n",
    "2. **Inferencia**: Realiza nueva inferencia de esquema en el batch actual\n",
    "3. **Merge**: Fusiona nuevas columnas al final del esquema existente\n",
    "4. **Almacenamiento**: Actualiza el esquema en `cloudFiles.schemaLocation`\n",
    "5. **Falla del stream**: El stream se detiene con `UnknownFieldException`\n",
    "6. **Reinicio**: Al reiniciar, usa el esquema evolucionado\n",
    "\n",
    "**Recomendación**: Configurar streams de Auto Loader con Lakeflow Jobs para reinicio automático tras cambios de esquema.\n",
    "\n",
    "#### Columna de datos rescatados (_rescued_data)\n",
    "\n",
    "Auto Loader automáticamente agrega una columna `_rescued_data` que contiene:\n",
    "- **Columnas nuevas** no previstas en el esquema\n",
    "- **Tipos de datos incompatibles**\n",
    "- **Diferencias de case** en nombres de columnas\n",
    "- **Ruta del archivo fuente** del registro\n",
    "\n",
    "```python\n",
    "# La columna contiene JSON con datos no parseados\n",
    "# Ejemplo: {\"new_column\": \"value\", \"source_file\": \"/path/to/file.csv\"}\n",
    "```\n",
    "\n",
    "#### Schema Hints para sobrescribir inferencia\n",
    "\n",
    "Puedes usar `schemaHints` para forzar tipos de datos específicos:\n",
    "\n",
    "```python\n",
    "(spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "    .option(\"cloudFiles.schemaHints\", \"fecha DATE, precio DECIMAL(10,2), tags MAP<STRING,STRING>\")\n",
    "    .load(source_path)\n",
    "    .writeStream...\n",
    ")\n",
    "```\n",
    "\n",
    "**Sintaxis de schemaHints**:\n",
    "- `columna TIPO` - Para tipos simples\n",
    "- `columna.nested TIPO` - Para campos anidados\n",
    "- `ARRAY<TIPO>` - Para arrays\n",
    "- `MAP<KEY_TYPE,VALUE_TYPE>` - Para maps\n",
    "- `STRUCT<campo1:TIPO1, campo2:TIPO2>` - Para structs\n",
    "\n",
    "### Ejemplo práctico - Schema Evolution en Auto Loader\n",
    "\n",
    "Basado en el notebook `02. carga_incremental_autoloader.ipynb`, vamos a mostrar cómo aplicar Schema Evolution a la tabla `account_premium_features` cargada con Auto Loader.\n",
    "\n",
    "#### Escenario\n",
    "\n",
    "Supongamos que los archivos CSV de `account_premium_features` comienzan a incluir una nueva columna `discount_applied` que indica si se aplicó un descuento. Queremos que nuestra tabla Delta evolucione automáticamente.\n",
    "\n",
    "#### Código de ejemplo con Schema Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83491aab-b5d5-4486-aa6e-413bc26596ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración igual que el notebook original\n",
    "\n",
    "# CARGA INCREMENTAL CON SCHEMA EVOLUTION EN AUTO LOADER\n",
    "query = (spark.readStream\n",
    "  .format(\"cloudFiles\")\n",
    "  \n",
    "  # Configuración de Auto Loader con evolución de esquema\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "  \n",
    "  # MODO DE EVOLUCIÓN: addNewColumns (default) - permite nuevas columnas\n",
    "  .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "  \n",
    "  # HABILITAR COLUMNA DE DATOS RESCATADOS para datos inesperados\n",
    "  .option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "  \n",
    "  # SCHEMA HINTS: forzar tipos específicos si es necesario\n",
    "  # .option(\"cloudFiles.schemaHints\", \"purchase_date DATE, amount_paid DECIMAL(10,2)\")\n",
    "  \n",
    "  .load(f\"{VOLUMEN_LANDING}/{TABLA}/\")\n",
    "  \n",
    "  # ESCRITURA CON SCHEMA EVOLUTION PARA DELTA TABLE\n",
    "  .writeStream\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "  .trigger(availableNow=True)\n",
    "  \n",
    "  # HABILITAR SCHEMA EVOLUTION PARA LA TABLA DELTA\n",
    "  .option(\"mergeSchema\", \"true\")  # <-- Permite nuevas columnas en tabla Delta\n",
    "  \n",
    "  .toTable(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "print(\"Carga incremental completada con Schema Evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31f10b5c-1ead-4cc4-a131-4209e112766c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ¿Qué habilita Schema Evolution en Auto Loader?\n",
    "\n",
    "**Configuración en Auto Loader (`cloudFiles.schemaEvolutionMode`):**\n",
    "- **`addNewColumns`**: Detecta nuevas columnas y las agrega al esquema (stream falla inicialmente)\n",
    "- **`rescue`**: Envía datos inesperados a columna `_rescued_data` sin fallar\n",
    "- **`failOnNewColumns`**: Falla completamente ante nuevas columnas\n",
    "- **`none`**: Ignora nuevas columnas completamente\n",
    "\n",
    "**Configuración en Delta Table (`mergeSchema`):**\n",
    "- Nuevas columnas se agregan automáticamente a la tabla Delta\n",
    "- Registros existentes tienen `NULL` en columnas nuevas\n",
    "- Compatible con type widening si está habilitado\n",
    "\n",
    "**Flujo completo de evolución:**\n",
    "1. **Auto Loader detecta** nuevas columnas en archivos CSV\n",
    "2. **Stream falla** con `UnknownFieldException` (modo `addNewColumns`)\n",
    "3. **Esquema se actualiza** en `cloudFiles.schemaLocation`\n",
    "4. **Al reiniciar**, el stream procesa con esquema evolucionado\n",
    "5. **Tabla Delta** agrega columnas automáticamente con `mergeSchema=true`\n",
    "\n",
    "**Comportamiento específico:**\n",
    "- Si un archivo CSV incluye `discount_applied`, Auto Loader lo detecta\n",
    "- Datos van inicialmente a `_rescued_data` o stream falla\n",
    "- Después del reinicio, `discount_applied` se agrega como columna regular\n",
    "- Registros anteriores tienen `NULL` en la nueva columna\n",
    "\n",
    "### Manejo de la columna _rescued_data\n",
    "\n",
    "La columna `_rescued_data` contiene datos que no pudieron ser parseados según el esquema actual:\n",
    "\n",
    "```sql\n",
    "-- Ver datos rescatados\n",
    "SELECT account_id, _rescued_data\n",
    "FROM account_premium_features\n",
    "WHERE _rescued_data IS NOT NULL\n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Contenido típico de _rescued_data:**\n",
    "- Nuevas columnas detectadas\n",
    "- Tipos de datos incompatibles\n",
    "- Diferencias de capitalización en nombres\n",
    "- Ruta del archivo fuente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbc5c299-455f-4f0c-922b-52b7fef67277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "Schema Enforcement y Schema Evolution son mecanismos complementarios para manejar la evolución de datos en entornos de producción:\n",
    "\n",
    "- **Schema Enforcement**: Proporciona validación estricta y consistencia\n",
    "- **Schema Evolution**: Permite adaptabilidad y resiliencia a cambios\n",
    "\n",
    "**Recomendaciones:**\n",
    "- Usar Schema Enforcement por defecto para calidad de datos\n",
    "- Habilitar Schema Evolution selectivamente donde sea necesario\n",
    "- Monitorear cambios de esquema en producción\n",
    "- Documentar evoluciones importantes\n",
    "- Probar cambios exhaustivamente antes de aplicar en producción"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03. schema_enforcement_evolution",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
