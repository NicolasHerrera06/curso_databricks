{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee21339-c0f9-426f-b2b0-b2ace9148b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de widgets para credenciales y parámetros\n",
    "\n",
    "# Widget para el host de la base de datos PostgreSQL\n",
    "dbutils.widgets.text(\"host\", \"datavision-dev.c340wcgc05b7.us-west-2.rds.amazonaws.com\", \"Host PostgreSQL\")\n",
    "\n",
    "# Widget para el usuario (tipo texto normal)\n",
    "dbutils.widgets.text(\"username\", \"dateneo_readonly\", \"Usuario BD\")\n",
    "\n",
    "# Widget para la contraseña (tipo password para seguridad)\n",
    "dbutils.widgets.text(\"password\", \"\", \"Contraseña BD\")\n",
    "\n",
    "# Widget para el nombre del alumno (usado en nombres de tablas)\n",
    "dbutils.widgets.text(\"alumno\", \"bruno\", \"Nombre del Alumno\")\n",
    "\n",
    "# Widget para el catálogo de destino en Databricks\n",
    "dbutils.widgets.text(\"catalogo\", \"bronce_dev\", \"Catálogo Destino\")\n",
    "\n",
    "# Leer los valores de los widgets\n",
    "HOST = dbutils.widgets.get(\"host\")\n",
    "USERNAME = dbutils.widgets.get(\"username\")\n",
    "PASSWORD = dbutils.widgets.get(\"password\")\n",
    "ALUMNO = dbutils.widgets.get(\"alumno\")\n",
    "CATALOGO = dbutils.widgets.get(\"catalogo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b209740a-8ff8-400f-b0a2-78ce7c6b655b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingesta de datos con JDBC en Apache Spark\n",
    "\n",
    "## ¿Qué es JDBC?\n",
    "\n",
    "JDBC (Java Database Connectivity) es una API de Java que permite conectar aplicaciones Java con bases de datos relacionales. En el contexto de Apache Spark, JDBC nos permite:\n",
    "\n",
    "- **Leer datos** desde bases de datos relacionales (PostgreSQL, MySQL, SQL Server, Oracle, etc.)\n",
    "- **Escribir datos** hacia estas bases de datos\n",
    "- **Realizar consultas SQL** directamente sobre las bases de datos fuente\n",
    "\n",
    "## JDBC en Spark\n",
    "\n",
    "Spark incluye un **data source JDBC** que facilita la conexión con bases de datos relacionales. Según la documentación oficial de Spark ([spark.apache.org/docs/latest/sql-data-sources-jdbc.html](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)):\n",
    "\n",
    "- **DataFrameReader.jdbc()**: Para leer datos desde JDBC\n",
    "- **DataFrameWriter.jdbc()**: Para escribir datos hacia JDBC\n",
    "- Soporta múltiples opciones de configuración como `url`, `dbtable`, `query`, `partitionColumn`, `numPartitions`, etc.\n",
    "\n",
    "### Ventajas del data source JDBC:\n",
    "- ✅ **Distribuido**: Spark puede paralelizar la lectura/escritura\n",
    "- ✅ **Escalable**: Maneja grandes volúmenes de datos eficientemente\n",
    "- ✅ **Flexible**: Soporta queries complejas y particionamiento\n",
    "- ✅ **Compatible**: Funciona con cualquier base de datos que tenga driver JDBC\n",
    "\n",
    "### Desventajas:\n",
    "- ❌ **Driver JDBC**: Debe estar disponible en el classpath de Spark\n",
    "- ❌ **Conexiones**: Cada partición requiere una conexión JDBC\n",
    "- ❌ **Rendimiento**: Puede ser más lento que conectores nativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5a04da9-4a3c-491f-84ab-e10caf0ede81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuración de la conexión JDBC\n",
    "\n",
    "### URL de conexión\n",
    "La URL JDBC sigue el formato estándar: `jdbc:<subprotocol>://<host>:<port>/<database>`\n",
    "\n",
    "En nuestro caso:\n",
    "- **Subprotocol**: `postgresql` (para PostgreSQL)\n",
    "- **Host**: servidor de base de datos\n",
    "- **Puerto**: `5432` (puerto estándar de PostgreSQL)\n",
    "- **Base de datos**: `datavision`\n",
    "\n",
    "### Propiedades de conexión\n",
    "Según la documentación de Spark, las propiedades principales son:\n",
    "\n",
    "| Propiedad | Descripción | Ejemplo |\n",
    "|-----------|-------------|---------|\n",
    "| `url` | URL completa de conexión JDBC | `jdbc:postgresql://host:5432/db` |\n",
    "| `dbtable` | Tabla o query a leer | `accounts` o `(SELECT * FROM accounts WHERE active=1) AS subq` |\n",
    "| `user` | Usuario de la base de datos | |\n",
    "| `password` | Contraseña del usuario | |\n",
    "| `driver` | Clase del driver JDBC | `org.postgresql.Driver` |\n",
    "\n",
    "### Opciones avanzadas importantes:\n",
    "- **`partitionColumn`**: Columna numérica para particionar la lectura\n",
    "- **`lowerBound` / `upperBound`**: Límites para el particionamiento\n",
    "- **`numPartitions`**: Número de particiones para paralelizar\n",
    "- **`fetchsize`**: Número de filas por fetch (mejora rendimiento)\n",
    "- **`batchsize`**: Tamaño del batch para escrituras\n",
    "\n",
    "### Driver JDBC\n",
    "El driver debe estar disponible en el classpath de Spark. Para PostgreSQL usamos `org.postgresql.Driver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd5befd-ddf3-4c5c-b6ee-c664cb853740",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURACIÓN DE LA CONEXIÓN JDBC\n",
    "# =============================================================================\n",
    "\n",
    "# Construcción de la URL JDBC para PostgreSQL\n",
    "# Formato: jdbc:postgresql://host:puerto/base_de_datos\n",
    "jdbc_url = f\"jdbc:postgresql://{HOST}:5432/datavision\"\n",
    "\n",
    "# Propiedades de conexión JDBC (según documentación de Spark)\n",
    "# Estas propiedades se pasan al driver JDBC\n",
    "properties = {\n",
    "    \"user\": USERNAME,                    # Usuario de la base de datos\n",
    "    \"password\": PASSWORD,               # Contraseña del usuario\n",
    "    \"driver\": \"org.postgresql.Driver\"   # Clase del driver JDBC de PostgreSQL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb45d46-0182-4b36-a000-85ae24fdbe66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Carga full idempotente\n",
    "\n",
    "### ¿Qué significa \"idempotente\"?\n",
    "\n",
    "Una operación es **idempotente** cuando puede ejecutarse múltiples veces sin cambiar el resultado final. En el contexto de ETL/ELT:\n",
    "\n",
    "- ✅ **Primera ejecución**: Inserta los datos\n",
    "- ✅ **Ejecuciones posteriores**: No duplica datos, mantiene consistencia\n",
    "- ✅ **Reintentos seguros**: Si falla, se puede re-ejecutar sin problemas\n",
    "\n",
    "### Estrategia de carga full idempotente\n",
    "\n",
    "Esta notebook implementa una **carga full idempotente** con la siguiente lógica:\n",
    "\n",
    "1. **Lectura completa**: Lee TODOS los datos de las tablas fuente via JDBC\n",
    "2. **Fecha de extracción**: Agrega columna `fecha_extraccion` con la fecha actual\n",
    "3. **Borrado selectivo**: Elimina SOLO los registros del día actual en destino\n",
    "4. **Inserción**: Agrega los nuevos datos en modo `append`\n",
    "\n",
    "```sql\n",
    "-- Pseudocódigo de lo que hace:\n",
    "DELETE FROM tabla_destino WHERE fecha_extraccion = '2026-01-01';\n",
    "INSERT INTO tabla_destino SELECT *, '2026-01-01' FROM tabla_fuente;\n",
    "```\n",
    "\n",
    "### Ventajas de esta estrategia:\n",
    "- ✅ **Idempotente**: Se puede ejecutar múltiples veces en el mismo día\n",
    "- ✅ **Histórico**: Mantiene datos de días anteriores\n",
    "- ✅ **Simple**: Lógica fácil de entender y mantener\n",
    "- ✅ **Consistente**: Datos del día siempre actualizados\n",
    "\n",
    "### Desventajas:\n",
    "- ❌ **No incremental**: Lee toda la tabla fuente cada vez\n",
    "- ❌ **Borrado completo del día**: Si hay cambios en fuente, se pierden\n",
    "- ❌ **Alto volumen**: Puede ser ineficiente para tablas muy grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abd616f9-9947-4989-b85e-118e1080e30e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 6"
    }
   },
   "outputs": [],
   "source": [
    "# Importaciones necesarias para el procesamiento de datos\n",
    "from pyspark.sql.functions import current_date  # Para agregar fecha de extracción\n",
    "from datetime import date  # Para obtener fecha actual en formato ISO\n",
    "from pyspark.sql.functions import col  # Para manipular columnas (no se usa aquí pero es común)\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURACIÓN DE LAS TABLAS A PROCESAR\n",
    "# =============================================================================\n",
    "\n",
    "# Lista de tuplas: (nombre_tabla_fuente, nombre_tabla_destino)\n",
    "# Cada tabla se lee desde PostgreSQL y se escribe en Delta Lake\n",
    "tables = [\n",
    "    (\"accounts\", f\"{CATALOGO}.datavision_{ALUMNO}.accounts\"),\n",
    "    (\"subscriptions\", f\"{CATALOGO}.datavision_{ALUMNO}.subscriptions\"),\n",
    "    (\"accounts_subscription\", f\"{CATALOGO}.datavision_{ALUMNO}.accounts_subscription\"),\n",
    "    (\"premium_features\", f\"{CATALOGO}.datavision_{ALUMNO}.premium_features\")\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# PROCESAMIENTO DE DATOS - CARGA FULL IDEMPOTENTE\n",
    "# =============================================================================\n",
    "\n",
    "# Diccionario para almacenar los DataFrames leídos\n",
    "dfs = {}\n",
    "\n",
    "# Obtener la fecha actual en formato ISO (YYYY-MM-DD)\n",
    "today = date.today().isoformat()\n",
    "\n",
    "# Procesar cada tabla definida en la lista\n",
    "for table, target in tables:\n",
    "    print(f\"Procesando tabla: {table} -> {target}\")\n",
    "\n",
    "    # 1. LECTURA DESDE JDBC\n",
    "    # spark.read.jdbc() utiliza el JDBC Data Source de Spark\n",
    "    # Lee TODOS los datos de la tabla fuente\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=table, properties=properties)\n",
    "    df_strings = (\n",
    "        df.select([col(c).cast(\"string\").alias(c) for c in df.columns])\n",
    "    )\n",
    "\n",
    "    # 2. AGREGAR FECHA DE EXTRACCIÓN\n",
    "    # Esto nos permite saber cuándo se extrajeron los datos\n",
    "    # y hacer la carga idempotente\n",
    "    df_strings = df_strings.withColumn(\"fecha_extraccion\", current_date())\n",
    "\n",
    "    # 3. ESCRITURA SELECTIVA EN MODO OVERWRITE CON replaceWhere\n",
    "    # Sobrescribe SOLO los registros del día actual para evitar duplicados\n",
    "    # Hace la carga idempotente y evita duplicados sin borrar datos históricos de otros días\n",
    "    (df_strings.write\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"replaceWhere\", f\"fecha_extraccion = '{today}'\")\n",
    "        .saveAsTable(target)\n",
    "    )\n",
    "\n",
    "    # Guardar el DataFrame en el diccionario para reporting\n",
    "    dfs[table] = df_strings\n",
    "\n",
    "# =============================================================================\n",
    "# REPORTING - MOSTRAR RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=== RESUMEN DE LA CARGA ===\")\n",
    "# Mostrar el conteo de registros procesados por tabla\n",
    "for table, df in dfs.items():\n",
    "    display(spark.createDataFrame([(table, df.count())], [\"Tabla\", \"Registros Procesados\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15e33ac6-9d90-4e40-9b8f-ca7a15d653ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ejercicio: Modificar la estrategia de carga\n",
    "\n",
    "## Objetivo\n",
    "Esta notebook implementa una **carga full idempotente**. Ahora te proponemos **modificar la estrategia** para explorar diferentes enfoques de carga de datos.\n",
    "\n",
    "## Alternativas de carga\n",
    "\n",
    "### A) Carga incremental sin borrado\n",
    "**Modificar la carga para que se carguen datos ante cada ejecución sin borrar nada**\n",
    "\n",
    "```python\n",
    "# En lugar de borrar por fecha simplemente hacer append siempre:\n",
    "df.write.mode(\"append\").saveAsTable(target)\n",
    "```\n",
    "\n",
    "**Preguntas para reflexionar:**\n",
    "- ¿Qué sucede si ejecuto la notebook múltiples veces?\n",
    "- ¿Cómo identifico qué datos son nuevos vs. duplicados?\n",
    "- ¿Cuándo sería útil esta estrategia?\n",
    "\n",
    "### B) Carga con overwrite completo\n",
    "**Hacer un overwrite completo de la tabla**\n",
    "\n",
    "```python\n",
    "# En lugar de borrar selectivo hacer overwrite completo:\n",
    "df.write.mode(\"overwrite\").saveAsTable(target)\n",
    "```\n",
    "\n",
    "**Preguntas para reflexionar:**\n",
    "- ¿Qué pasa con los datos históricos?\n",
    "- ¿Es idempotente esta operación?\n",
    "- ¿Cuándo usaría overwrite en lugar de append?\n",
    "\n",
    "### C) Merge por clave primaria\n",
    "**Merge por clave primaria de la tabla (más avanzado)**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Leer datos existentes\n",
    "existing_df = spark.read.table(target)\n",
    "\n",
    "# Hacer merge basado en clave primaria\n",
    "# (pseudocódigo - requeriría implementar lógica de merge)\n",
    "# MERGE INTO {target} t\n",
    "# USING (SELECT *, '{today}' as fecha_extraccion FROM fuente) s\n",
    "# ON t.primary_key = s.primary_key\n",
    "# WHEN MATCHED THEN UPDATE SET ...\n",
    "# WHEN NOT MATCHED THEN INSERT ...\n",
    "```\n",
    "\n",
    "**Preguntas para reflexionar:**\n",
    "- ¿Qué necesito saber de la tabla para implementar merge?\n",
    "- ¿Cómo identifico la clave primaria?\n",
    "- ¿Cuáles son las ventajas sobre las estrategias anteriores?\n",
    "\n",
    "## Instrucciones del ejercicio\n",
    "\n",
    "1. **Elige una alternativa** de las tres presentadas\n",
    "2. **Explica por qué** elegiste esa alternativa considerando:\n",
    "   - El caso de uso específico\n",
    "   - Ventajas y desventajas\n",
    "   - Requisitos de idempotencia\n",
    "3. **Implementa la modificación** copiando esta notebook\n",
    "4. **Prueba tu solución** ejecutándola múltiples veces\n",
    "5. **Documenta los resultados** y qué aprendiste"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01. carga_full_jdbc",
   "widgets": {
    "alumno": {
     "currentValue": "nicolas_herrera",
     "nuid": "39a93768-2664-4ee8-81ee-5ba08f0f397b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bruno",
      "label": "Nombre del Alumno",
      "name": "alumno",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bruno",
      "label": "Nombre del Alumno",
      "name": "alumno",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalogo": {
     "currentValue": "bronce_dev",
     "nuid": "f47be835-2564-454e-b699-d4c99e1dfb94",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronce_dev",
      "label": "Catálogo Destino",
      "name": "catalogo",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronce_dev",
      "label": "Catálogo Destino",
      "name": "catalogo",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "host": {
     "currentValue": "datavision-dev.c340wcgc05b7.us-west-2.rds.amazonaws.com",
     "nuid": "863b47bd-ad98-41d0-91a5-c35528228c9f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "datavision-dev.c340wcgc05b7.us-west-2.rds.amazonaws.com",
      "label": "Host PostgreSQL",
      "name": "host",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "datavision-dev.c340wcgc05b7.us-west-2.rds.amazonaws.com",
      "label": "Host PostgreSQL",
      "name": "host",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "password": {
     "currentValue": "DateneoRead123!",
     "nuid": "c57c9d4b-c9a9-454a-901e-e4d8511f0904",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Contraseña BD",
      "name": "password",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Contraseña BD",
      "name": "password",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "username": {
     "currentValue": "dateneo_readonly",
     "nuid": "70516a6c-c305-4c6a-bcde-1bb5f7193c8c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dateneo_readonly",
      "label": "Usuario BD",
      "name": "username",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dateneo_readonly",
      "label": "Usuario BD",
      "name": "username",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
