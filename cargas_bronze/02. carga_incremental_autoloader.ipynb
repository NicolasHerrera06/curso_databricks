{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f68eedf-8494-4344-b71a-007d27bf1b88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Carga Incremental con Auto Loader en Databricks\n",
    "\n",
    "## ¿Qué es Auto Loader?\n",
    "\n",
    "Auto Loader es una herramienta de Databricks que permite cargar datos de manera automática e incremental desde almacenamiento de objetos en la nube (como S3, Azure Blob Storage o Google Cloud Storage) hacia tablas Delta Lake.\n",
    "\n",
    "### ¿Por qué usar Auto Loader?\n",
    "\n",
    "Auto Loader tiene sentido usarlo en combinación con herramientas de extracción y carga de datos como **Fivetran** o **dltHub**. Estas herramientas se encargan de:\n",
    "\n",
    "1. **Extraer datos** desde sistemas origen (bases de datos, APIs, etc.)\n",
    "2. **Depositar archivos** en un directorio de almacenamiento de objetos\n",
    "3. **Auto Loader** se encarga de detectar automáticamente estos archivos nuevos y cargarlos incrementalmente a Databricks\n",
    "\n",
    "En nuestro caso específico, estamos usando un **script automatizado sencillo** que extrae datos diariamente desde una base de datos PostgreSQL. El script implementa una **carga incremental por cursor** usando la columna `purchase_date` como cursor para detectar cambios:\n",
    "\n",
    "```sql\n",
    "-- Extraer datos de ayer\n",
    "extract_date = (datetime.now(timezone.utc).date() - timedelta(days=1)).isoformat()\n",
    "where_clause = f\"WHERE purchase_date::date = DATE '{extract_date}'\"\n",
    "\n",
    "SELECT\n",
    "    account_feature_id,\n",
    "    account_id,\n",
    "    feature_id,\n",
    "    purchase_date,\n",
    "    amount_paid\n",
    "FROM public.account_premium_features\n",
    "{where_clause}\n",
    "```\n",
    "\n",
    "El **cursor** en este caso es la columna `purchase_date`: cada día el script extrae solo los registros del día anterior, asegurando que no se reprocesen datos históricos y que la carga sea incremental.\n",
    "\n",
    "Este patrón crea un **pipeline EL completo**:\n",
    "- **Extract**: Script automatizado extrae datos diarios\n",
    "- **Load**: Archivos CSV se depositan en el directorio de landing. Auto Loader detecta y carga incrementalmente a Delta Lake\n",
    "\n",
    "### Características principales de Auto Loader:\n",
    "\n",
    "- **Detección automática de archivos nuevos**: Auto Loader monitorea continuamente el directorio de origen y detecta automáticamente cuando llegan archivos nuevos\n",
    "- **Procesamiento incremental**: Solo procesa los archivos que no han sido procesados anteriormente, evitando reprocesar datos ya cargados\n",
    "- **Manejo de esquemas**: Puede inferir el esquema automáticamente o usar esquemas evolucionados\n",
    "- **Checkpointing**: Mantiene el estado del procesamiento para asegurar que no se pierdan datos en caso de fallos\n",
    "- **Soporte para múltiples formatos**: CSV, JSON, Parquet, Avro, entre otros\n",
    "\n",
    "### ¿Cómo detecta Auto Loader los archivos nuevos?\n",
    "\n",
    "Auto Loader soporta dos modos principales para detectar archivos nuevos en el directorio de origen:\n",
    "\n",
    "#### 1. **Directory Listing Mode** (Modo de listado de directorios)\n",
    "- **Cómo funciona**: Auto Loader lista periódicamente el directorio de entrada para identificar archivos nuevos\n",
    "- **Ventajas**: Fácil de configurar, no requiere permisos adicionales más allá del acceso a los datos\n",
    "- **Limitaciones**: Puede ser menos eficiente en directorios muy grandes o con muchos archivos nuevos\n",
    "- **Optimización**: En Databricks Runtime 9.1+, Auto Loader detecta automáticamente si los archivos llegan con orden léxico y reduce las llamadas a la API\n",
    "\n",
    "#### 2. **File Notification Mode** (Modo de notificación de archivos - recomendado)\n",
    "- **Cómo funciona**: Utiliza servicios de notificación y cola de la infraestructura cloud para suscribirse a eventos de archivos\n",
    "- **Ventajas**: Más performante y escalable que el modo directory listing\n",
    "- **Configuración**: Auto Loader puede configurar automáticamente el servicio de notificaciones si habilitas eventos de archivos en la ubicación externa\n",
    "- **Recomendación**: Databricks recomienda este modo para la mayoría de las cargas de trabajo\n",
    "\n",
    "**Nota importante**: Puedes cambiar entre modos de detección durante reinicios del stream manteniendo las garantías de procesamiento exactly-once.\n",
    "\n",
    "### ¿Cómo funciona?\n",
    "\n",
    "Auto Loader utiliza un patrón de **Structured Streaming** para procesar los archivos. Structured Streaming es el motor de procesamiento de streams de Apache Spark que permite tratar flujos de datos como si fueran tablas SQL.\n",
    "\n",
    "En Structured Streaming, definimos una consulta que se ejecuta continuamente sobre datos que llegan en tiempo real. En el caso de Auto Loader, esta consulta:\n",
    "1. Lee archivos desde el directorio de origen\n",
    "2. Los procesa según el formato especificado\n",
    "3. Escribe los resultados a una tabla Delta\n",
    "4. Mantiene un checkpoint para recordar qué archivos ya fueron procesados\n",
    "\n",
    "### Carga incremental basada en detección de cambios\n",
    "\n",
    "En esta notebook implementamos una **carga incremental** que combina dos estrategias complementarias:\n",
    "\n",
    "1. **Detección de archivos nuevos**: Auto Loader detecta automáticamente archivos nuevos en el directorio de origen\n",
    "2. **Checkpointing de procesamiento**: Los checkpoints mantienen el estado de qué archivos ya fueron procesados\n",
    "\n",
    "Esta carga se ejecuta en modo `availableNow=True`, procesando todos los datos disponibles en el momento y luego deteniéndose (no es un stream continuo).\n",
    "\n",
    "Referencias:\n",
    "- [Documentación oficial de Auto Loader](https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/)\n",
    "- [Guía de Structured Streaming](https://spark.apache.org/docs/latest/streaming/getting-started.html#programming-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "794645f3-1132-407e-9197-fac2b1e776c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuración de la carga incremental\n",
    "\n",
    "En esta sección configuramos los parámetros necesarios para la carga incremental. Utilizamos **widgets** de Databricks para hacer la notebook parametrizable y reutilizable.\n",
    "\n",
    "### Parámetros configurables:\n",
    "\n",
    "- **Alumno**: Identificador del estudiante (se utiliza para crear namespaces únicos)\n",
    "- **Catálogo**: Catálogo de Databricks donde se creará la tabla destino\n",
    "- **Volumen landing**: Directorio de origen donde llegan los archivos CSV nuevos\n",
    "- **Volumen bronce**: Directorio de destino en la capa bronze del data lake\n",
    "- **Tabla**: Nombre de la tabla que se creará en el catálogo\n",
    "\n",
    "Los widgets permiten ejecutar la misma notebook con diferentes configuraciones sin modificar el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c81c95e-ff54-457b-84a1-8bfee9d03add",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de parámetros mediante widgets\n",
    "# Los widgets permiten parametrizar la notebook para diferentes entornos y usuarios\n",
    "\n",
    "# Creamos los widgets para configurar la carga\n",
    "dbutils.widgets.text(\"alumno\", \"bruno\", \"Nombre del alumno\")\n",
    "dbutils.widgets.text(\"catalogo\", \"bronce_dev\", \"Catálogo de destino\")\n",
    "dbutils.widgets.text(\"volumen_landing\", \"/Volumes/landing_dev/postgres_datavision/landing-volume-postgres-datavision/\", \"Directorio de origen (landing)\")\n",
    "dbutils.widgets.text(\"volumen_bronce\", \"\", \"Directorio de destino (bronce)\")\n",
    "dbutils.widgets.text(\"tabla\", \"account_premium_features\", \"Nombre de la tabla a crear\")\n",
    "\n",
    "# Leemos los valores de los widgets\n",
    "ALUMNO = dbutils.widgets.get(\"alumno\")\n",
    "CATALOGO = dbutils.widgets.get(\"catalogo\")\n",
    "VOLUMEN_LANDING = dbutils.widgets.get(\"volumen_landing\")\n",
    "\n",
    "# Si no se especifica volumen_bronce, lo construimos automáticamente\n",
    "VOLUMEN_BRONCE_INPUT = dbutils.widgets.get(\"volumen_bronce\")\n",
    "if VOLUMEN_BRONCE_INPUT.strip() == \"\":\n",
    "    VOLUMEN_BRONCE = f\"/Volumes/{CATALOGO}/datavision/bronce-volume-datavision\"\n",
    "else:\n",
    "    VOLUMEN_BRONCE = VOLUMEN_BRONCE_INPUT\n",
    "\n",
    "TABLA = dbutils.widgets.get(\"tabla\")\n",
    "\n",
    "# Mostramos la configuración final\n",
    "print(f\"Configuración de carga:\")\n",
    "print(f\"- Alumno: {ALUMNO}\")\n",
    "print(f\"- Catálogo: {CATALOGO}\")\n",
    "print(f\"- Volumen landing: {VOLUMEN_LANDING}\")\n",
    "print(f\"- Volumen bronce: {VOLUMEN_BRONCE}\")\n",
    "print(f\"- Tabla: {TABLA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "960896ce-6c7c-4743-a274-86248bf93277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ejecución de la carga incremental\n",
    "\n",
    "En esta sección implementamos la **carga incremental** utilizando Auto Loader y Structured Streaming.\n",
    "\n",
    "### ¿Qué hace este código?\n",
    "\n",
    "1. **Checkpointing**: Creamos una ruta para guardar el estado del procesamiento. Los checkpoints son archivos que recuerdan qué archivos ya fueron procesados, permitiendo reanudar la carga desde donde se quedó.\n",
    "\n",
    "2. **Lectura con Auto Loader**: Utilizamos `spark.readStream.format(\"cloudFiles\")` para configurar Auto Loader:\n",
    "   - `cloudFiles.format`: Especifica que los archivos son CSV\n",
    "   - `cloudFiles.schemaLocation`: Define dónde guardar el esquema inferido automáticamente\n",
    "\n",
    "3. **Escritura estructurada**: Configuramos la escritura del stream:\n",
    "   - `checkpointLocation`: Mantiene el estado del procesamiento\n",
    "   - `trigger(availableNow=True)`: Procesa todos los datos disponibles y se detiene (no continuo)\n",
    "   - `toTable()`: Escribe directamente a una tabla Delta Lake\n",
    "\n",
    "### Ventajas de este enfoque:\n",
    "\n",
    "- **Incremental**: Solo procesa archivos nuevos\n",
    "- **Fault-tolerant**: Los checkpoints permiten recuperarse de fallos\n",
    "- **Escalable**: Puede manejar grandes volúmenes de datos\n",
    "- **Automático**: Detecta nuevos archivos sin intervención manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1150b83a-e6e2-44a3-8baf-4540e4aacc06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Structured Streaming y procesamiento incremental\n",
    "\n",
    "### ¿Qué es Structured Streaming?\n",
    "\n",
    "Structured Streaming es el framework de Apache Spark para procesar datos en tiempo real de manera estructurada. A diferencia de otros sistemas de streaming que procesan eventos uno por uno, Structured Streaming trata los flujos de datos como si fueran **tablas SQL infinitas** que se actualizan continuamente.\n",
    "\n",
    "**Modelo de programación de Structured Streaming:**\n",
    "- Definimos una consulta SQL-like sobre un flujo de datos\n",
    "- Spark se encarga de ejecutar la consulta de manera incremental y eficiente\n",
    "- Los resultados se actualizan automáticamente cuando llegan nuevos datos\n",
    "\n",
    "### Checkpointing en Structured Streaming\n",
    "\n",
    "En el contexto de Structured Streaming, el **checkpointing** es fundamental para la carga incremental:\n",
    "\n",
    "1. **Checkpoint**: Un marcador persistente que indica qué archivos ya fueron procesados\n",
    "2. **Recuperación de fallos**: Permite reanudar el procesamiento desde el último punto exitoso\n",
    "3. **Consistencia**: Garantiza que cada archivo se procesa exactamente una vez\n",
    "\n",
    "**Ventajas del checkpointing:**\n",
    "- **Eficiencia**: No reprocesamos datos ya cargados\n",
    "- **Confiabilidad**: Los checkpoints permiten recuperarse de fallos\n",
    "- **Escalabilidad**: Puede manejar grandes volúmenes sin degradación del rendimiento\n",
    "- **Simplicidad**: El framework maneja automáticamente la complejidad del estado\n",
    "\n",
    "### Cómo funciona en Auto Loader:\n",
    "\n",
    "1. **Detección**: Auto Loader monitorea el directorio de origen\n",
    "2. **Procesamiento**: Solo procesa archivos nuevos desde el último checkpoint\n",
    "3. **Estado**: Actualiza el checkpoint después de procesar exitosamente\n",
    "4. **Consistencia**: Garantiza que cada archivo se procesa exactamente una vez\n",
    "\n",
    "Esta combinación de Auto Loader + Structured Streaming nos da una solución robusta y eficiente para cargas incrementales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d97a3675-312f-4c04-ae12-6a4769fd53cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Configuración del checkpoint para mantener el estado del procesamiento\n",
    "# El checkpoint es crucial para la carga incremental - guarda qué archivos ya fueron procesados\n",
    "checkpoint_path = f\"{VOLUMEN_BRONCE}/_checkpoint_{ALUMNO}/{TABLA}\"\n",
    "\n",
    "# Iniciamos la consulta de Structured Streaming con Auto Loader\n",
    "# Esta es la configuración principal de la carga incremental\n",
    "query = (spark.readStream\n",
    "  # Especificamos que usaremos el formato cloudFiles (Auto Loader)\n",
    "  .format(\"cloudFiles\")\n",
    "\n",
    "  # Configuramos el formato de los archivos de origen (CSV en este caso)\n",
    "  .option(\"cloudFiles.format\", \"csv\")\n",
    "\n",
    "  # Especificamos dónde guardar la información del esquema inferido\n",
    "  # Esto permite manejar cambios en el esquema de manera automática\n",
    "  .option(\"cloudFiles.schemaLocation\", checkpoint_path)\n",
    "\n",
    "  # Definimos el directorio de origen donde llegan los archivos nuevos\n",
    "  .load(f\"{VOLUMEN_LANDING}/{TABLA}/\")\n",
    "  .withColumn(\"fecha_carga\", current_date())\n",
    "\n",
    "  # Configuramos la escritura de la stream\n",
    "  .writeStream\n",
    "\n",
    "  # Especificamos dónde guardar los checkpoints del procesamiento\n",
    "  # Los checkpoints contienen el estado de qué archivos ya fueron procesados\n",
    "  .option(\"checkpointLocation\", checkpoint_path)\n",
    "\n",
    "  # Usamos trigger availableNow para procesar todos los datos disponibles\n",
    "  # y luego detener la consulta (no es un stream continuo)\n",
    "  .trigger(availableNow=True)\n",
    "\n",
    "  # Especificamos la tabla destino donde se escribirán los datos\n",
    "  .toTable(f\"{CATALOGO}.datavision_{ALUMNO}.{TABLA}\"))\n",
    "\n",
    "# La consulta se ejecuta automáticamente cuando se define con toTable()\n",
    "# Esto inicia el procesamiento incremental de los archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4caa2aa-bb4e-4f4f-a613-c7ce0d03f738",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Verificación de la carga\n",
    "\n",
    "Después de ejecutar la carga incremental, es importante verificar que los datos se cargaron correctamente. La consulta SQL a continuación cuenta los registros en la tabla destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9db450a-aaee-4af5-b212-5459f2b1df4f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768510881864}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT count(*) FROM bronce_dev.datavision_nicolas_herrera.account_premium_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f9ada80-cb1d-4d13-b068-87490299f533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ejercicio Práctico: Experimentando con Checkpointing en Auto Loader\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Ahora que entendiste cómo funciona Auto Loader, el checkpointing y la carga incremental, vamos a realizar **dos experimentos controlados** para analizar el comportamiento del sistema y extraer conclusiones importantes sobre cómo maneja el estado y la detección de cambios.\n",
    "\n",
    "## Experimento 1: Recrear la tabla y ejecutar nuevamente\n",
    "\n",
    "### Consigna:\n",
    "1. **Ejecuta primero la notebook completa** con la configuración por defecto para cargar la tabla `account_premium_features`\n",
    "2. **Verifica la cantidad de registros** cargados inicialmente\n",
    "3. **Borra la tabla** con este comando SQL:\n",
    "   ```sql\n",
    "   DROP TABLE IF EXISTS {CATALOGO}.datavision_{ALUMNO}.account_premium_features;\n",
    "   ```\n",
    "4. **Ejecuta la notebook nuevamente** (sin modificar ningún parámetro). La tabla se debe recrear automaticamente.\n",
    "5. **Compara los resultados**:\n",
    "   - ¿Se cargaron los mismos registros?\n",
    "   - ¿Cuántos registros hay ahora en la tabla?\n",
    "   - ¿Qué conclusiones puedes extraer sobre el comportamiento del checkpoint?\n",
    "\n",
    "## Experimento 2: Borrar el checkpoint y ejecutar nuevamente\n",
    "\n",
    "### Consigna:\n",
    "1. **Ejecuta primero la notebook completa** nuevamente\n",
    "2. **Verifica la cantidad de registros** actuales\n",
    "3. **Borra el directorio de checkpoint** ejecutando este comando:\n",
    "   ```python\n",
    "   checkpoint_path = f\"{VOLUMEN_BRONCE}/_checkpoint_{ALUMNO}/account_premium_features\"\n",
    "   dbutils.fs.rm(checkpoint_path, recurse=True)\n",
    "   print(f\"Checkpoint borrado: {checkpoint_path}\")\n",
    "   ```\n",
    "4. **Ejecuta la notebook nuevamente** (sin modificar ningún parámetro)\n",
    "5. **Compara los resultados**:\n",
    "   - ¿Se duplicaron los datos?\n",
    "   - ¿Cuántos registros hay ahora en la tabla?\n",
    "   - ¿Qué sucedió con el procesamiento?\n",
    "\n",
    "## Análisis y conclusiones\n",
    "\n",
    "Después de completar ambos experimentos, responde estas preguntas:\n",
    "\n",
    "### Experimento 1 (Recrear tabla):\n",
    "- ¿Por qué al recrear la tabla se volvieron a cargar todos los datos?\n",
    "- ¿Qué rol juega la existencia de la tabla en el proceso de carga?\n",
    "- ¿Cómo se comporta Auto Loader cuando la tabla destino ya existe vs cuando no existe?\n",
    "\n",
    "### Experimento 2 (Borrar checkpoint):\n",
    "- ¿Qué sucedió cuando borramos el checkpoint?\n",
    "- ¿Se duplicaron los datos o se reprocesaron los archivos?\n",
    "- ¿Cuál es la diferencia entre \"checkpoint\" y \"estado de la tabla\"?\n",
    "- ¿Por qué es importante el checkpoint para la carga incremental?\n",
    "\n",
    "### Conclusiones generales:\n",
    "- ¿Cuál es la relación entre el checkpoint y la detección de archivos ya procesados?\n",
    "- ¿En qué escenarios sería peligroso borrar un checkpoint?\n",
    "- ¿Cómo garantiza exactamente-once processing el sistema de checkpointing?\n",
    "- ¿Qué ventajas tiene este mecanismo comparado con un sistema sin checkpoints?"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6437129495422294,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02. carga_incremental_autoloader",
   "widgets": {
    "alumno": {
     "currentValue": "nicolas_herrera",
     "nuid": "b3a34883-7f49-4ebf-b431-453bfecf34fc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bruno",
      "label": "Nombre del alumno",
      "name": "alumno",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bruno",
      "label": "Nombre del alumno",
      "name": "alumno",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalogo": {
     "currentValue": "bronce_dev",
     "nuid": "b0de7ecc-6938-4a4f-b9ef-f9c908db0e3c",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronce_dev",
      "label": "Catálogo de destino",
      "name": "catalogo",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronce_dev",
      "label": "Catálogo de destino",
      "name": "catalogo",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "tabla": {
     "currentValue": "account_premium_features",
     "nuid": "bfb151b5-d1ca-43a3-8ffb-a0134df9b0d4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "account_premium_features",
      "label": "Nombre de la tabla a crear",
      "name": "tabla",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "account_premium_features",
      "label": "Nombre de la tabla a crear",
      "name": "tabla",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volumen_bronce": {
     "currentValue": "",
     "nuid": "7054b7fb-537d-435e-8441-7cfd386bcfa5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Directorio de destino (bronce)",
      "name": "volumen_bronce",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Directorio de destino (bronce)",
      "name": "volumen_bronce",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volumen_landing": {
     "currentValue": "/Volumes/landing_dev/postgres_datavision/landing-volume-postgres-datavision/",
     "nuid": "22b24b68-62a6-4f8d-beb0-33ca8ee5b1c5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/landing_dev/postgres_datavision/landing-volume-postgres-datavision/",
      "label": "Directorio de origen (landing)",
      "name": "volumen_landing",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/landing_dev/postgres_datavision/landing-volume-postgres-datavision/",
      "label": "Directorio de origen (landing)",
      "name": "volumen_landing",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
