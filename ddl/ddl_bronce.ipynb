{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28a81295-98a4-4674-9d91-ff224926e9e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creación de Esquemas y Tablas - Capa Bronce\n",
    "\n",
    "Esta notebook crea las **estructuras de datos base** (esquemas y tablas) en la **capa Bronce** del Data Lakehouse usando Databricks.\n",
    "\n",
    "## ¿Qué hace esta notebook?\n",
    "\n",
    "1. **Configura parámetros dinámicos** mediante widgets para nombres de esquemas y catálogos\n",
    "2. **Define y ejecuta DDL** para crear 5 tablas que existen en el transaccional de DataVision:\n",
    "   - `accounts`: Información básica de cuentas de usuario\n",
    "   - `account_premium_features`: Features premium adquiridos por cuenta\n",
    "   - `accounts_subscription`: Suscripciones activas por cuenta\n",
    "   - `premium_features`: Catálogo de features premium disponibles\n",
    "   - `subscriptions`: Tipos de suscripción disponibles\n",
    "\n",
    "---\n",
    "\n",
    "## Configuración con widgets\n",
    "\n",
    "### ¿Qué son los widgets en Databricks?\n",
    "\n",
    "Los **widgets** son controles interactivos que permiten parametrizar notebooks. Se usan para evitar valores hardcodeados y facilitar la reutilización del código.\n",
    "\n",
    "Beneficios:\n",
    "- Configurables sin tocar el código\n",
    "- Útiles para pruebas y entornos educativos\n",
    "- Permiten ejecutar la misma notebook con distintos valores\n",
    "- Compatibles con jobs programados\n",
    "\n",
    "### Widgets usados en esta notebook\n",
    "\n",
    "- **`alumno`**: Nombre del alumno (para nombres de tablas)\n",
    "- **`catalogo`**: Catálogo de destino en Databricks \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ee21339-c0f9-426f-b2b0-b2ace9148b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuración de widgets para parámetros\n",
    "# Los widgets permiten cambiar estos valores sin modificar el código\n",
    "\n",
    "# Widget para el nombre del alumno (usado en nombres de esquema)\n",
    "dbutils.widgets.text(\"alumno\", \"bruno\", \"Nombre del Alumno\")\n",
    "\n",
    "# Widget para el catálogo de destino en Databricks\n",
    "dbutils.widgets.text(\"catalogo\", \"bronce_dev\", \"Catálogo Destino\")\n",
    "\n",
    "ALUMNO = dbutils.widgets.get(\"alumno\")\n",
    "CATALOGO = dbutils.widgets.get(\"catalogo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85931543-c068-41a2-a648-ecd8b331617f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Definición y ejecución de DDL (Data Definition Language)\n",
    "\n",
    "La siguiente celda contiene la **lógica principal** de creación de tablas. Esta celda:\n",
    "\n",
    "### ¿Qué hace?\n",
    "\n",
    "1. **Define las estructuras de tabla** como strings SQL usando f-strings para incluir los parámetros dinámicos (`CATALOGO` y `ALUMNO`)\n",
    "2. **Crea 5 tablas relacionadas** con un sistema de gestión de cuentas de usuario\n",
    "3. **Ejecuta los DDL** mediante un bucle que itera sobre cada definición de tabla\n",
    "4. **Usa `CREATE TABLE IF NOT EXISTS`** para evitar errores si las tablas ya existen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a5af81-6d8d-4f8b-97a6-83508366cbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definir los DDL como tuplas\n",
    "ddls = (\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOGO}.datavision_{ALUMNO}.accounts (\n",
    "        account_id STRING,\n",
    "        account_name STRING,\n",
    "        email STRING,\n",
    "        created_at STRING,\n",
    "        updated_at STRING,\n",
    "        fecha_extraccion DATE\n",
    "    )\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOGO}.datavision_{ALUMNO}.account_premium_features (\n",
    "        account_feature_id STRING,\n",
    "        account_id STRING,\n",
    "        feature_id STRING,\n",
    "        purchase_date STRING,\n",
    "        amount_paid STRING,\n",
    "        fecha_carga DATE,\n",
    "        _rescued_data STRING\n",
    "    )\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE  {CATALOGO}.datavision_{ALUMNO}.accounts_subscription (\n",
    "        account_subscription_id STRING,\n",
    "        account_id STRING,\n",
    "        subscription_id STRING,\n",
    "        start_date STRING,\n",
    "        end_date STRING,\n",
    "        fecha_extraccion DATE\n",
    "    )\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOGO}.datavision_{ALUMNO}.premium_features (\n",
    "        feature_id STRING,\n",
    "        feature_name STRING,\n",
    "        description STRING,\n",
    "        base_price STRING,\n",
    "        created_at STRING,\n",
    "        updated_at STRING,\n",
    "        fecha_extraccion DATE\n",
    "    )\n",
    "    \"\"\",\n",
    "    f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOGO}.datavision_{ALUMNO}.subscriptions (\n",
    "        subscription_id STRING,\n",
    "        subscription_name STRING,\n",
    "        max_contents_per_month STRING,\n",
    "        created_at STRING,\n",
    "        updated_at STRING,\n",
    "        fecha_extraccion DATE\n",
    "    )\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Ejecutar los DDL\n",
    "for ddl in ddls:\n",
    "    spark.sql(ddl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6bc4622-7b0b-4cc4-b377-77b1c367874f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Mejores prácticas en la capa Bronce\n",
    "\n",
    "### 1. Campos como STRING por diseño\n",
    "\n",
    "En la **capa Bronze** (capa raw), todos los campos se definen como `STRING` **intencionalmente**:\n",
    "\n",
    "**¿Por qué?**\n",
    "- **Tolerancia a datos sucios**: Evita errores de parsing cuando llegan datos inesperados\n",
    "- **Flexibilidad máxima**: Permite cargar cualquier tipo de dato sin validaciones estrictas\n",
    "- **Schema evolution**: Facilita agregar nuevas columnas o cambiar tipos de datos\n",
    "- **Debugging simplificado**: Los datos \"raros\" no rompen el proceso de carga\n",
    "\n",
    "**Transformación posterior**: En capas superiores (plata/oro) se harán las conversiones y validaciones apropiadas.\n",
    "\n",
    "### 2. Versionado y migraciones de Esquema\n",
    " \n",
    "**¿Qué es una migración de esquema?**\n",
    "\n",
    "Una **migración de esquema** es el proceso de modificar estructuralmente la base de datos (por ejemplo, agregar/quitar tablas, columnas, tipos de datos, constraints, etc.) de manera controlada y versionada. Las migraciones permiten que los cambios en el esquema estén documentados en archivos de código, puedan replicarse fácilmente en diferentes entornos y se integren con el flujo de desarrollo (CI/CD).\n",
    "\n",
    "Por ejemplo: Si un día necesitás agregar una columna nueva o cambiar el tipo de dato de un campo, en vez de hacerlo manualmente, creás un archivo de migración donde definís ese cambio y el sistema de migraciones aplica ese script en cada ambiente, evitando inconsistencias.\n",
    "\n",
    "**Nota:** En este curso **no vamos a ver a fondo herramientas ni técnicas de migraciones de esquema**, pero te lo mencionamos porque en proyectos reales es una práctica fundamental para asegurar el orden y la trazabilidad de los cambios estructurales en la base de datos.\n",
    "\n",
    "**Herramientas populares para migración y versionado de esquemas:**\n",
    " \n",
    "- **[Flyway](https://www.red-gate.com/products/flyway/community/)**: Una herramienta de migración de esquemas open-source que soporta una amplia variedad de sistemas de bases de datos. Permite definir migraciones con SQL o Java, y se puede integrar fácilmente en procesos de build y despliegue.\n",
    "\n",
    "- **[Liquibase](https://github.com/liquibase/liquibase-databricks/tree/main)**: Otra herramienta open-source para rastrear, gestionar y aplicar cambios en el esquema de la base de datos. Soporta múltiples formatos para definir los cambios: XML, YAML, JSON y SQL.\n",
    " \n",
    "Ambas herramientas también ofrecen versiones Enterprise/Pro con funcionalidades avanzadas como detección de desvíos (\"drift detection\"), control de cambios por políticas/reglas, aplicaciones con interfaz gráfica, soporte premium y mucho más.\n",
    "\n",
    "- **[Guía oficial Databricks](https://community.databricks.com/t5/technical-blog/databricks-schema-versioning-with-flyway-and-liquibase-a-step-by/ba-p/90504)** \n",
    "\n",
    "**Beneficios de las migraciones SQL:**\n",
    "- **Versionado del esquema**: Cada cambio queda registrado en código\n",
    "- **Rollback seguro**: Posibilidad de revertir cambios problemáticos\n",
    "- **Entornos consistentes**: Dev, staging y prod tienen el mismo esquema\n",
    "- **Auditoría**: Historial completo de cambios en la estructura de datos\n",
    "- **Colaboración**: Los cambios de esquema siguen el flujo de CI/CD"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ddl_bronce",
   "widgets": {
    "alumno": {
     "currentValue": "nicolas_herrera",
     "nuid": "39a93768-2664-4ee8-81ee-5ba08f0f397b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bruno",
      "label": "Nombre del Alumno",
      "name": "alumno",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bruno",
      "label": "Nombre del Alumno",
      "name": "alumno",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalogo": {
     "currentValue": "bronce_dev",
     "nuid": "f47be835-2564-454e-b699-d4c99e1dfb94",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronce_dev",
      "label": "Catálogo Destino",
      "name": "catalogo",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronce_dev",
      "label": "Catálogo Destino",
      "name": "catalogo",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
